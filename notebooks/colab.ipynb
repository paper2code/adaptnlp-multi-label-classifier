{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdaptNLP multi-label classifier - PapersWithCode dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKB369aRSIpC",
        "colab_type": "text"
      },
      "source": [
        "# AdaptNLP - Multi-Label Classifier - PapersWithCode Dataset\n",
        "In this notebook, we will try to predict the tasks of paper's abstract based on the paperwithcode dataset.\n",
        "\n",
        "References:\n",
        "- https://github.com/Novetta/adaptnlp\n",
        "- https://github.com/paperswithcode/paperswithcode-data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADuYMUzsSr0J",
        "colab_type": "text"
      },
      "source": [
        "## 1. Google Colaboratory\n",
        "\n",
        "Google Colaboratory is a hosted Python development environment based on [Jupyter notebooks](https://jupyter.org/). These notebooks allow to interweave structured text (using the [markdown syntax](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)) and Python code. They and are particularly suited for prototyping new ideas, due to interactive code execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2kWzlbbS9Um",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Activate GPU support\n",
        "\n",
        "To activate GPU support, click on `Runtime > Change runtime type` int he notebook menu and choose `GPU` as the hardware accelerator. To check whether the GPU is available for computation, we import the deep learning framework [PyTorch](https://pytorch.org/):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d478oG9USHNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wBLsJkpTHVQ",
        "colab_type": "text"
      },
      "source": [
        "If successful, the output of the cell above should print `True`. Note that Google Colaboratory also offers [TPU](https://cloud.google.com/tpu/) support. These *Tensor Processing Units* are specifically designed for machine learning tasks and may outperform conventional GPUs. While support for TPUs in PyTorch is still pending, [tensorflow](https://www.tensorflow.org/) models may benefit from using TPUs (see [this tutorial](https://colab.research.google.com/notebooks/tpu.ipynb)).\n",
        "\n",
        "### 1.2 Useful commands\n",
        "\n",
        "Within the notebook environment, you can not only execute Python code, but also bash commands by prepending a `!`. For example, you can install new Python packages via the package manager `pip`. Here, we just check the installed version of PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1jgJ4ofTMMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiGHoxlfTQX2",
        "colab_type": "text"
      },
      "source": [
        "Another useful command is `!kill -9 -1`. It will reset all running kernels and free up memory (including GPU memory). Furthermore, there are a few commands to have a closer look on the hardware spcifications, i.e. to get information about the installed CPU and GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHpAt1C6TUQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!lscpu |grep 'Model name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TScHEdUTZeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUVRuwKYTgMK",
        "colab_type": "text"
      },
      "source": [
        "In addition, you can check the available RAM and HDD memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHcJUi2uTaDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /proc/meminfo | grep 'MemAvailable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_hggUdwTjGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!df -h / | awk '{print $4}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dWYrjwVTlJL",
        "colab_type": "text"
      },
      "source": [
        "Finally, one can execute the following command to get a live update on the GPU usage. This is useful to check how much of the GPU memory is in use to optimize the batchsize for training. Note that whenever the training routine in a notebook is still running, you need to execute this command in another Colaboratory notebook to get an instant response:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBLJD2HhTos8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwAT6Rw6TtoG",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Mount Google Drive\n",
        "\n",
        "Another important prerequisite for training our neural network is a place to save checkpoints of the trained model and to store obtained training data. Colaboratory provides convenient access to Google Drive via the `google.colab` Python module. The following command will mount your Google Drive contents to the folder path `/content/gdrive` on the Colaboratory instance. For authentication, you have to click the generated link and paste the authorization code into the input field:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W_COrPrTvQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bf795a8-6c14-453f-d1cb-e012c42f1fb7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzA9SjM2T1pF",
        "colab_type": "text"
      },
      "source": [
        "Now you can use conventional Python packages such as `os` or `sys` to create/delete/change files in your Google Drive folders just as if you were working on your local machine.\n",
        "\n",
        "## 2. Getting training data\n",
        "\n",
        "We will fetch the paperswithcode's dataset, being updated daily, via a their direct link. For more information, please visit their dedicated repository:\n",
        "- https://github.com/paperswithcode/paperswithcode-data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUptLKKtUPvj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "c2d0a43c-b56a-4634-8a72-ec15a0a57d9f"
      },
      "source": [
        "%rm -f papers-with-abstracts.json.gz*\n",
        "!wget -nc https://paperswithcode.com/media/about/papers-with-abstracts.json.gz\n",
        "!gunzip -f papers-with-abstracts.json.gz\n",
        "!ls -lhS\n",
        "!head -n 30 papers-with-abstracts.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-14 11:38:57--  https://paperswithcode.com/media/about/papers-with-abstracts.json.gz\n",
            "Resolving paperswithcode.com (paperswithcode.com)... 172.67.73.69, 104.26.13.155, 104.26.12.155, ...\n",
            "Connecting to paperswithcode.com (paperswithcode.com)|172.67.73.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78863047 (75M) [application/octet-stream]\n",
            "Saving to: ‘papers-with-abstracts.json.gz’\n",
            "\n",
            "papers-with-abstrac 100%[===================>]  75.21M  12.3MB/s    in 7.3s    \n",
            "\n",
            "2020-09-14 11:39:06 (10.3 MB/s) - ‘papers-with-abstracts.json.gz’ saved [78863047/78863047]\n",
            "\n",
            "total 245M\n",
            "-rw-r--r-- 1 root root 245M Sep 13 20:09 papers-with-abstracts.json\n",
            "drwx------ 4 root root 4.0K Sep 14 11:14 gdrive\n",
            "drwxr-xr-x 1 root root 4.0K Aug 27 16:39 sample_data\n",
            "[\n",
            "  {\n",
            "    \"paper_url\": \"https://paperswithcode.com/paper/understanding-the-semantic-intent-of-natural\",\n",
            "    \"arxiv_id\": null,\n",
            "    \"title\": \"Understanding the Semantic Intent of Natural Language Query\",\n",
            "    \"abstract\": \"\",\n",
            "    \"url_abs\": \"https://www.aclweb.org/anthology/I13-1063/\",\n",
            "    \"url_pdf\": \"https://www.aclweb.org/anthology/I13-1063\",\n",
            "    \"proceeding\": \"IJCNLP 2013 10\",\n",
            "    \"authors\": [\n",
            "      \"Juan Xu\",\n",
            "      \"Qi Zhang\",\n",
            "      \"Xuanjing Huang\"\n",
            "    ],\n",
            "    \"tasks\": [],\n",
            "    \"date\": \"2013-10-01\"\n",
            "  },\n",
            "  {\n",
            "    \"paper_url\": \"https://paperswithcode.com/paper/ec-eccc2e-13a1eae-c-ea-c-c-a-study-on-voice\",\n",
            "    \"arxiv_id\": null,\n",
            "    \"title\": \"\\u904b\\u7528\\u985e\\u795e\\u7d93\\u7db2\\u8def\\u65b9\\u6cd5\\u4e4b\\u8a9e\\u8a00\\u7aef\\u9ede\\u5075\\u6e2c\\u7814\\u7a76 (A Study on Voice Activation Detection by Using Neural Networks) [In Chinese]\",\n",
            "    \"abstract\": \"\",\n",
            "    \"url_abs\": \"https://www.aclweb.org/anthology/O17-1002/\",\n",
            "    \"url_pdf\": \"https://www.aclweb.org/anthology/O17-1002\",\n",
            "    \"proceeding\": \"ROCLINGIJCLCLP 2017 11\",\n",
            "    \"authors\": [\n",
            "      \"Yu-Chih Deng\",\n",
            "      \"Chen-Yu Chiang\",\n",
            "      \"Chen-Ming Pan\"\n",
            "    ],\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pby5UTBsWVdG",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Getting AdaptNLP package\n",
        "\n",
        "<p align=\"center\">\n",
        "    <a href=\"https://github.com/Novetta/adaptnlp\"> <img src=\"https://raw.githubusercontent.com/novetta/adaptnlp/master/docs/img/NovettaAdaptNLPlogo-400px.png\" width=\"400\"/></a>\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "<strong> A high level framework and library for running, training, and deploying state-of-the-art Natural Language Processing (NLP) models for end to end tasks.</strong>\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "    <a href=\"https://circleci.com/gh/Novetta/adaptnlp\">\n",
        "        <img src=\"https://img.shields.io/circleci/build/github/Novetta/adaptnlp/master\">\n",
        "    </a>\n",
        "    <a href=\"https://badge.fury.io/py/adaptnlp\">\n",
        "        <img src=\"https://badge.fury.io/py/adaptnlp.svg\">\n",
        "    </a>\n",
        "    <a href=\"https://github.com/Novetta/adaptnlp/blob/master/LICENSE\">\n",
        "        <img src=\"https://img.shields.io/github/license/novetta/adaptnlp\">\n",
        "    </a>\n",
        "</p>\n",
        "\n",
        "\n",
        "AdaptNLP allows users ranging from beginner python coders to experienced machine learning engineers to leverage\n",
        "state-of-the-art NLP models and training techniques in one easy-to-use python package.\n",
        "\n",
        "Built atop Zalando Research's Flair and Hugging Face's Transformers library, AdaptNLP provides Machine\n",
        "Learning Researchers and Scientists a modular and **adaptive** approach to a variety of NLP tasks with an\n",
        "**Easy** API for training, inference, and deploying NLP-based microservices.\n",
        "\n",
        "**Key Features**\n",
        "\n",
        "  - **[Full Guides and API Documentation](https://novetta.github.io/adaptnlp)**\n",
        "  - [Tutorial](https://github.com/Novetta/adaptnlp/tree/master/tutorials) Jupyter/Google Colab Notebooks\n",
        "  - Unified API for NLP Tasks with SOTA Pretrained Models (Adaptable with Flair and Transformer's Models)\n",
        "    - Token Tagging \n",
        "    - Sequence Classification\n",
        "    - Embeddings\n",
        "    - Question Answering\n",
        "    - Summarization\n",
        "    - Translation\n",
        "    - Text Generation\n",
        "    - <em> More in development </em>\n",
        "  - Training and Fine-tuning Interface\n",
        "    - Integration with Transformer's Trainer Module for fast and easy transfer learning with custom datasets\n",
        "    - Jeremy's **[ULM-FIT](https://arxiv.org/abs/1801.06146)** approach for transfer learning in NLP\n",
        "    - Fine-tuning Transformer's language models and task-specific predictive heads like Flair's `SequenceClassifier`\n",
        "  - [Rapid NLP Model Deployment](https://github.com/Novetta/adaptnlp/tree/master/rest) with Sebastián's [FastAPI](https://github.com/tiangolo/fastapi) Framework\n",
        "    - Containerized FastAPI app\n",
        "    - Immediately deploy any custom trained Flair or AdaptNLP model\n",
        "  - [Dockerizing AdaptNLP with GPUs](https://hub.docker.com/r/achangnovetta/adaptnlp)\n",
        "    - Easily build and run AdaptNLP containers leveraging NVIDIA GPUs with Docker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y97tcgJuW_yt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af35cb4e-6e8b-49d4-f585-3cb6a88d6595"
      },
      "source": [
        "!pip install git+https://github.com/Novetta/adaptnlp"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Novetta/adaptnlp\n",
            "  Cloning https://github.com/Novetta/adaptnlp to /tmp/pip-req-build-_38de75j\n",
            "  Running command git clone -q https://github.com/Novetta/adaptnlp /tmp/pip-req-build-_38de75j\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from adaptnlp==0.2.0) (7.1.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from adaptnlp==0.2.0) (1.0.0)\n",
            "Collecting jupyterlab\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/a9/d7c904ee406d1ce320fd1d91e05111fa158e66bb217f68d070b5f58c5937/jupyterlab-2.2.8-py3-none-any.whl (7.8MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2.0.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp==0.2.0) (1.6.0+cu101)\n",
            "Collecting transformers<4.0.0,>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.4MB/s \n",
            "\u001b[?25hCollecting nlp<1.0.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e3/bcdc59f3434b224040c1047769c47b82705feca2b89ebbc28311e3764782/nlp-0.4.0-py3-none-any.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 43.2MB/s \n",
            "\u001b[?25hCollecting flair<1.0.0,>=0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/82/898d26ae4c7a8c2cb51dfc776c7b323b049981a08300d811e18ac12825a8/flair-0.6.0.post1-py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 39.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from adaptnlp==0.2.0) (1.0.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (4.7.7)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (5.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->adaptnlp==0.2.0) (5.3.1)\n",
            "Requirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp==0.2.0) (2.11.2)\n",
            "Requirement already satisfied: tornado!=6.0.0,!=6.0.1,!=6.0.2 in /usr/local/lib/python3.6/dist-packages (from jupyterlab->adaptnlp==0.2.0) (5.1.1)\n",
            "Collecting jupyterlab-server<2.0,>=1.1.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/eb/560043dcd8376328f8b98869efed66ef68307278406ab99c7f63a34d4ae2/jupyterlab_server-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.4.0->adaptnlp==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.4.0->adaptnlp==0.2.0) (1.18.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 47.0MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.8MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 48.8MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/99/0a605f016121ca314d1469dc9069e4978395bc46fda40f73099d90ad3ba4/pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 156kB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp<1.0.0,>=0.4.0->adaptnlp==0.2.0) (0.3.2)\n",
            "Collecting pytest>=5.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/2c/7e29215cb19745ad67b6476b73fd1299872563f28329ea01d9d887713aaf/pytest-6.0.2-py3-none-any.whl (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 46.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (3.2.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.1.2)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/4b/f29cb8cf226d49b14f0d18bd175916dd0054dc66e42488f2c808075ef5a8/konoha-4.6.1-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (2.8.1)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (3.6.0)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.6MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 44.0MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 42.3MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/ed/b2b072c1d53388931e626ca0816cdb8e03416ad9a440c8d9e0e060859f41/Janome-0.4.0-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.8.7)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (4.2.6)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<2.0.0,>=1.0.0->adaptnlp==0.2.0) (2018.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->adaptnlp==0.2.0) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->adaptnlp==0.2.0) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->adaptnlp==0.2.0) (4.3.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (0.3)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (5.0.7)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (3.1.5)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (1.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->adaptnlp==0.2.0) (4.6.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp==0.2.0) (19.0.2)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->adaptnlp==0.2.0) (1.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->adaptnlp==0.2.0) (3.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->adaptnlp==0.2.0) (1.0.18)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->adaptnlp==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->adaptnlp==0.2.0) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.10->jupyterlab->adaptnlp==0.2.0) (1.1.1)\n",
            "Collecting jsonschema>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hCollecting json5\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/81/22bf51a5bc60dde18bb6164fd597f18ee683de8670e141364d9c432dd3cf/json5-0.9.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<4.0.0,>=3.0.0->adaptnlp==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.10.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.9.0)\n",
            "Collecting pluggy<1.0,>=0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.0.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (20.2.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (2.5)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (3.11.0)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (2.1.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.12.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp==0.2.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp==0.2.0) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp==0.2.0) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp==0.2.0) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->adaptnlp==0.2.0) (50.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->adaptnlp==0.2.0) (0.5.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->adaptnlp==0.2.0) (0.6.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->jupyterlab-server<2.0,>=1.1.5->jupyterlab->adaptnlp==0.2.0) (0.17.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (3.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.14.59)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.59 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (1.17.59)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.59->boto3->smart-open>=1.2.1->gensim>=3.4.0->flair<1.0.0,>=0.6.0->adaptnlp==0.2.0) (0.15.2)\n",
            "Building wheels for collected packages: adaptnlp, sacremoses, sqlitedict, segtok, ftfy, mpld3, langdetect, overrides\n",
            "  Building wheel for adaptnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for adaptnlp: filename=adaptnlp-0.2.0-cp36-none-any.whl size=82287 sha256=e5f32c18aacc3c5dc87b7f9a0010af69a716a3bf028041b82cf5e6ef4e2ba428\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n4ji1xk4/wheels/6b/8a/39/f2973ce520f67fc11e1047c2bddb9975600d8be8a62f999b0e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=476883eb645013deb0b30929dae38ddfe9833409480fdee3a0246d12ba2f7561\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=369d357cb925e64ba44ca96cc948ee41d6773ee1697d7f54413a78fae623cea4\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=972674730377cf830559bb26a3a0bb2eb3f7243c6071ad3d4806a344d89a80b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=4b68028a8058959b41de31e7b5d8b3ffffd203027e3c16367a6b5e7dafb516a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=e60b4ef9082dc7f67c59cdea108522dcb1f51b1efc20e23962955ab1a79a8eff\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=dd9890dccc55538261b9107fc6868e07bd4fe1cf91355a795e9eb3ec009bf5fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=18fb18b6ba40d2ef547c6391cb8d08cd9dffd674140f6b1ae30d3238ef4bddad\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "Successfully built adaptnlp sacremoses sqlitedict segtok ftfy mpld3 langdetect overrides\n",
            "\u001b[31mERROR: nbclient 0.5.0 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jsonschema, json5, jupyterlab-server, jupyterlab, sentencepiece, sacremoses, tokenizers, transformers, xxhash, pyarrow, nlp, pluggy, pytest, sqlitedict, overrides, konoha, segtok, ftfy, mpld3, deprecated, langdetect, bpemb, janome, flair, adaptnlp\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed adaptnlp-0.2.0 bpemb-0.3.2 deprecated-1.2.10 flair-0.6.0.post1 ftfy-5.8 janome-0.4.0 json5-0.9.5 jsonschema-3.2.0 jupyterlab-2.2.8 jupyterlab-server-1.2.0 konoha-4.6.1 langdetect-1.0.8 mpld3-0.3 nlp-0.4.0 overrides-3.0.0 pluggy-0.13.1 pyarrow-1.0.1 pytest-6.0.2 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.7.0 tokenizers-0.8.1rc2 transformers-3.1.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CldzzAxsX3yf",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Preparing train data\n",
        "\n",
        "We will have to convert the json dataset to a csv file (commas separated) with the following columns:\n",
        "```\n",
        "Content,Labels\n",
        "\"Fifty-four patients had pancreas cancer, confirmed by resection or biopsy in all cases .\",outcome/population\n",
        "```\n",
        "\n",
        "The label in the target of the data will be separated by \"/\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd0CguI_YwGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "37e5d87f-e616-4afa-9854-7fff51f48971"
      },
      "source": [
        "!pip install pandas\n",
        "!pip install tqdm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.49.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbGKIaj2Yh_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "from tqdm import tqdm\n",
        "\n",
        "import json\n",
        "import os.path\n",
        "\n",
        "file_name = \"papers-with-abstracts.json\"\n",
        "cols = ['content', 'tasks']\n",
        "df = pd.DataFrame(columns=cols)\n",
        "\n",
        "with open(file_name, encoding='utf-8') as f:\n",
        "    docs = json.load(f)\n",
        "    for doc in docs:\n",
        "      # print(doc)\n",
        "      if doc['title'] != '' and len(doc['tasks']) > 0:\n",
        "        lst_dict=({'content': doc['title'], 'tasks': \",\".join(doc['tasks'])})\n",
        "        df = df.append(lst_dict, ignore_index=True)\n",
        "\n",
        "df.to_csv('papers-with-abstracts.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlBc19hwZezB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "09887912-7323-4453-dd4b-fb5774e12bcb"
      },
      "source": [
        "!head -n 35 papers-with-abstracts.csv\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "content,tasks\n",
            "\"Commonsense knowledge relations are crucial for advanced NLU tasks. We examine the learnability of such relations as represented in CONCEPTNET, taking into account their specific properties, which can make relation classification difficult: a given concept pair can be linked by multiple relation types, and relations can have multi-word arguments of diverse semantic types. We explore a neural open world multi-label classification approach that focuses on the evaluation of classification accuracy for individual relations. Based on an in-depth study of the specific properties of the CONCEPTNET resource, we investigate the impact of different relation representations and model variations. Our analysis reveals that the complexity of argument types and relation ambiguity are the most important challenges to address. We design a customized evaluation method to address the incompleteness of the resource that can be expanded in future work.\",Multi-Label Classification/Relation Classification\n",
            "\"Deep neural networks are data hungry models and thus face difficulties when\n",
            "attempting to train on small text datasets. Transfer learning is a potential\n",
            "solution but their effectiveness in the text domain is not as explored as in\n",
            "areas such as image analysis. In this paper, we study the problem of transfer\n",
            "learning for text summarization and discuss why existing state-of-the-art\n",
            "models fail to generalize well on other (unseen) datasets. We propose a\n",
            "reinforcement learning framework based on a self-critic policy gradient\n",
            "approach which achieves good generalization and state-of-the-art results on a\n",
            "variety of datasets. Through an extensive set of experiments, we also show the\n",
            "ability of our proposed framework to fine-tune the text summarization model\n",
            "using only a few training samples. To the best of our knowledge, this is the\n",
            "first work that studies transfer learning in text summarization and provides a\n",
            "generic solution that works well on unseen data.\",Text Summarization/Transfer Learning/Transfer Reinforcement Learning\n",
            "\"Cross-modal retrieval between visual data and natural language description remains a long-standing challenge in multimedia. While recent image-text retrieval methods offer great promise by learning deep representations aligned across modalities, most of these meth-ods are plagued by the issue of training with small-scale datasets covering a limited number of images with ground-truth sentences. Moreover, it is extremely expensive to create a larger dataset by annotating millions of training images with ground-truth sentences and may lead to a biased model. Inspired by the recent success of web-supervised learning in deep neural networks, we capitalize on readily-available web images with noisy annotations to learn robust image-text joint representation. Specifically, our main idea is to leverage web images and corresponding tags, along with fully annotated datasets, in training for learning the visual-semantic joint embedding. We propose a two-stage approach for the task that can augment a typical supervised pair-wise ranking loss based formulation with weakly-annotated web images to learn a more robust visual-semantic embedding. Extensive experiments on two standard benchmark datasets demonstrate that our method achieves a significant performance gain in image-text retrieval compared to state-of-the-art approaches.\",Cross-Modal Retrieval\n",
            "\"Sentence formation is a highly structured, history-dependent, and\n",
            "sample-space reducing (SSR) process. While the first word in a sentence can be\n",
            "chosen from the entire vocabulary, typically, the freedom of choosing\n",
            "subsequent words gets more and more constrained by grammar and context, as the\n",
            "sentence progresses. This sample-space reducing property offers a natural\n",
            "explanation of Zipf's law in word frequencies, however, it fails to capture the\n",
            "structure of the word-to-word transition probability matrices of English text.\n",
            "Here we adopt the view that grammatical constraints (such as\n",
            "subject--predicate--object) locally re-order the word order in sentences that\n",
            "are sampled with a SSR word generation process. We demonstrate that\n",
            "superimposing grammatical structure -- as a local word re-ordering\n",
            "(permutation) process -- on a sample-space reducing process is sufficient to\n",
            "explain both, word frequencies and word-to-word transition probabilities. We\n",
            "compare the quality of the grammatically ordered SSR model in reproducing\n",
            "several test statistics of real texts with other text generation models, such\n",
            "as the Bernoulli model, the Simon model, and the Monkey typewriting model.\",Text Generation\n",
            "\"State-of-the-art end-to-end automatic speech recognition (ASR) extracts acoustic features from input speech signal every 10 ms which corresponds to a frame rate of 100 frames/second. In this report, we investigate the use of high-frame-rate features extraction in end-to-end ASR. High frame rates of 200 and 400 frames/second are used in the features extraction and provide additional information for end-to-end ASR. The effectiveness of high-frame-rate features extraction is evaluated independently and in combination with speed perturbation based data augmentation. Experiments performed on two speech corpora, Wall Street Journal (WSJ) and CHiME-5, show that using high-frame-rate features extraction yields improved performance for end-to-end ASR, both independently and in combination with speed perturbation. On WSJ corpus, the relative reduction of word error rate (WER) yielded by high-frame-rate features extraction independently and in combination with speed perturbation are up to 21.3% and 24.1%, respectively. On CHiME-5 corpus, the corresponding relative WER reductions are up to 2.8% and 7.9%, respectively, on the test data recorded by microphone arrays and up to 11.8% and 21.2%, respectively, on the test data recorded by binaural microphones.\",Data Augmentation/End-To-End Speech Recognition/Speech Recognition\n",
            "\"Automatic language identification is a natural language processing problem\n",
            "that tries to determine the natural language of a given content. In this paper\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 85279 entries, 0 to 85278\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   content  85017 non-null  object\n",
            " 1   tasks    85279 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}